{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "from random import shuffle\n",
    "\n",
    "from pytz import country_timezones\n",
    "\n",
    "import pytz\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from datalab.context import Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO IMPORT LOGS FROM S3 TO GCP USING BATCH OF 10000 LOGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#connect with S3\n",
    "AWS_KEY = <YOUR_AWS_KEY>\n",
    "AWS_SECRET = <YOUR_AWS_SECRET>\n",
    "conn = boto.connect_s3(AWS_KEY,AWS_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get specific bucket\n",
    "specific_bucket = conn.get_bucket(<YOUR_BUCKET>)\n",
    "\n",
    "#Get files from specific prefix\n",
    "#bucket_files = specific_bucket.list(prefix='2016/08/01/4155902955122327623')\n",
    "bucket_files = specific_bucket.list(prefix='2016/10/20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b_files = [log_file for log_file in bucket_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get only files without bug recordings\n",
    "new_files = b_files[0:68517]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_files = new_files[20000:30000]\n",
    "\n",
    "conc_contents = \"[\"\n",
    "\n",
    "for log_file in subset_files:\n",
    "    \n",
    "    if subset_files.index(log_file) !=0:\n",
    "      conc_contents = conc_contents + ','  \n",
    "      \n",
    "    preconts = log_file.read()\n",
    "    pre_cont1 = preconts.strip('\\n')\n",
    "    pre_cont2 = re.split(r\"}\\n{\",pre_cont1)\n",
    "    pre_cont3 = '},{'.join(pre_cont2)\n",
    "    conc_contents = conc_contents + pre_cont3\n",
    "conc_contents = conc_contents + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ok_contents = json.loads(conc_contents) \n",
    "#Converting to DataFrame\n",
    "df_contents = pd.DataFrame(ok_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df_contents.apply(lambda x : tuple(x) if type(x) is list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making preprocessing to handle df methods\n",
    "features = ['error_source','get','post','referrer_user','referrer_user_guid_all','referring_user','user']\n",
    "\n",
    "features = [column for column in df.columns if column in features]\n",
    "\n",
    "for feature in features:\n",
    "  df[feature] = df[feature].apply(lambda x : tuple(x) if type(x) is list else x)\n",
    "#df.user.apply(lambda x : tuple(x) if type(x) is list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#droping irrelevant features\n",
    "drop_features = ['app_release','bluetooth_enabled','bluetooth_version',\n",
    "                'entity_type','event','forced_login','get',\n",
    "                'guid_game','handle_url','nfc','post','referrer_user',\n",
    "                'referrer_user_guid_all','referring_user','server']\n",
    "\n",
    "drop_features = [column for column in df.columns if column in drop_features]\n",
    "\n",
    "df = df.drop(drop_features,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(axis=1,how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['_type'].str.startswith('slow:') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting timestamp into date/time \n",
    "time_conv = pd.to_datetime(df['time'],unit='s')\n",
    "time_data = pd.DatetimeIndex(time_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['day'] = time_data.day\n",
    "df['dayofweek'] = time_data.dayofweek\n",
    "df['dayofyear'] = time_data.dayofyear\n",
    "df['month'] = time_data.month\n",
    "df['daysinmonth'] = time_data.daysinmonth\n",
    "df['year'] = time_data.year\n",
    "df['hour'] = time_data.hour\n",
    "df['minutes'] = time_data.minute\n",
    "df['seconds'] = time_data.second\n",
    "df['weekofyear'] = time_data.weekofyear\n",
    "df['time_data'] = time_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Every empty field will be filled with 777 code\n",
    "#df = df.replace([np.NAN],[777])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = df.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-8c58854e48ea>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-8c58854e48ea>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    bucket_name = <YOUR_BUCKET>\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "project = Context.default().project_id\n",
    "bucket_name = <YOUR_BUCKET>\n",
    "bucket_path = 'gs://' + bucket_name + <YOUR_PREFIX>\n",
    "bucket_object = bucket_path + 'YOUR_LOG_FILE'\n",
    "\n",
    "print 'Bucket: ' + bucket_path\n",
    "print 'Object: ' + bucket_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%storage write --variable json_file --object $bucket_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Fields that need to be preprocessed from the dataset.\n",
    "#error_source,get,post (check in more detail), referrer_user (check in more detail), referrer_user_guid_all (check in more detail),referring_user,user"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
